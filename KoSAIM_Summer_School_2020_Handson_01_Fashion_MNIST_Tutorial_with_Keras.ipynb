{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WsAx4toWxKZE"
   },
   "source": [
    "![대체 텍스트](https://www.pyimagesearch.com/wp-content/uploads/2019/02/fashion_mnist_dataset_sample.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xhfmxR8o2sWd"
   },
   "source": [
    "\n",
    "# MNIST is too easy.\n",
    "Convolutional nets can achieve 99.7% on MNIST. Classic machine learning algorithms can also achieve 97% easily. \n",
    "# MNIST is overused. \n",
    "In this April 2017 Twitter thread, Google Brain research scientist and deep learning expert Ian Goodfellow calls for people to move away from MNIST.\n",
    "# MNIST can not represent modern CV tasks.\n",
    "\n",
    "# Fashion MNIST dataset\n",
    "Similar to the MNIST digit dataset, the Fashion MNIST dataset includes:\n",
    "\n",
    "60,000 training examples\n",
    "\n",
    "10,000 testing examples\n",
    "\n",
    "10 classes\n",
    "\n",
    "28×28 grayscale/single channel images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sFD7RLgA3ouu"
   },
   "source": [
    "![대체 텍스트](https://www.pyimagesearch.com/wp-content/uploads/2019/02/fashion_mnist_obtaining.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OFK9236vMmZO"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.utils import np_utils\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Activation\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "\n",
    "from keras import optimizers\n",
    "\n",
    "from keras import backend as K\n",
    "#K.set_image_dim_ordering('tf')\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "emDMGh2lWBHd"
   },
   "source": [
    "#STEP 1: Fashion MNIST 데이터 읽어들이기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1gjzm6goU1rp"
   },
   "outputs": [],
   "source": [
    "from keras.datasets import fashion_mnist\n",
    "((trainX, trainY), (testX, testY)) = fashion_mnist.load_data()\n",
    "\n",
    "# initialize the label names\n",
    "labelNames = [\"top\", \"trouser\", \"pullover\", \"dress\", \"coat\", \n",
    "             \"sandal\", \"shirt\", \"sneaker\", \"bag\", \"ankle boot\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8RwCuia7X_gD"
   },
   "source": [
    "#STEP 2: 데이터 살펴보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YFgKtGe4YFoT"
   },
   "outputs": [],
   "source": [
    "plt_row = 5\n",
    "plt_col = 5\n",
    "\n",
    "width = height = 28\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (15,15)\n",
    "\n",
    "f, axarr = plt.subplots(plt_row, plt_col)\n",
    "\n",
    "for i in range(plt_row*plt_col):\n",
    "\n",
    "    sub_plt = axarr[int(i/plt_row), i%plt_col]\n",
    "    sub_plt.axis('off')\n",
    "    sub_plt.imshow(testX[i].reshape(width, height), cmap='gray')\n",
    "    sub_plt_title = 'R: ' + labelNames[testY[i]]\n",
    "    sub_plt.set_title(sub_plt_title)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aPIIgjLbc9Cu"
   },
   "source": [
    "#STEP 3: 딥러닝을 위한 데이터 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "q9Lx0-DQdD93"
   },
   "outputs": [],
   "source": [
    "# flatten 28*28 images to a 784 vector for each image\n",
    "width = height = 28\n",
    "num_pixels = width * height\n",
    "trainX = trainX.reshape(60000, num_pixels).astype('float32') / 255.0\n",
    "testX = testX.reshape(10000, num_pixels).astype('float32') / 255.0\n",
    "\n",
    "# 훈련셋과 검증셋 분리\n",
    "valX = trainX[50000:]\n",
    "valY = trainY[50000:]\n",
    "trainX = trainX[:50000]\n",
    "trainY = trainY[:50000]\n",
    "\n",
    "# one hot encode outputs\n",
    "num_classes = 10\n",
    "trainY = np_utils.to_categorical(trainY, num_classes)\n",
    "valY = np_utils.to_categorical(valY, num_classes)\n",
    "testY = np_utils.to_categorical(testY, num_classes)\n",
    "\n",
    "print ('train shape: \\t', trainX.shape)\n",
    "print ('valid shape: \\t', valX.shape)\n",
    "print ('test shape: \\t', testX.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wtemFso_KR6o"
   },
   "outputs": [],
   "source": [
    "trainX.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6BhD2uuPfgZG"
   },
   "source": [
    "#STEP 4: 첫번째 인공지능 모델 (퍼셉트론)\n",
    "\n",
    "![대체 텍스트](https://www.simplilearn.com/ice9/free_resources_article_thumb/diagram-of-a-biological-neuron.jpg)\n",
    "\n",
    "![대체 텍스트](http://bit.ly/2ldH0Bg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "H-P5NCZHfxNs"
   },
   "outputs": [],
   "source": [
    "def logistic_regression_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Dense(num_classes, input_dim=num_pixels, kernel_initializer='normal', activation='softmax'))\n",
    "    \n",
    "    # compile model\n",
    "    sgd = optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DVwdrbt2gyWg"
   },
   "source": [
    "#STEP 5: 첫번째 인공지능 모델 학습!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gCgT7q9dgx6C"
   },
   "outputs": [],
   "source": [
    "# build the model\n",
    "model = logistic_regression_model()\n",
    "model.summary()\n",
    "# fix random seed for reproductibility\n",
    "seed = 7\n",
    "np.random.seed(seed)\n",
    "\n",
    "# FIT THE MODEL - OPTIMIZATION\n",
    "hist = model.fit(trainX, trainY, validation_data=(valX, valY), epochs=20, batch_size=64, verbose=2)\n",
    "model.save('logistic_regression_model.h5')\n",
    "\n",
    "# 학습과정 살펴보기\n",
    "fig, loss_ax = plt.subplots()\n",
    "\n",
    "acc_ax = loss_ax.twinx()\n",
    "\n",
    "loss_ax.plot(hist.history['loss'], 'y', label='train loss')\n",
    "loss_ax.plot(hist.history['val_loss'], 'r', label='val loss')\n",
    "loss_ax.set_ylim([0.0, 1.5])\n",
    "\n",
    "acc_ax.plot(hist.history['accuracy'], 'b', label='train acc')\n",
    "acc_ax.plot(hist.history['val_accuracy'], 'g', label='val acc')\n",
    "acc_ax.set_ylim([0.5, 1.0])\n",
    "\n",
    "loss_ax.set_xlabel('epoch')\n",
    "loss_ax.set_ylabel('loss')\n",
    "acc_ax.set_ylabel('accuray')\n",
    "\n",
    "loss_ax.legend(loc='upper left')\n",
    "acc_ax.legend(loc='lower left')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "J_i3PjlRjRSY"
   },
   "source": [
    "#STEP 6: 결과 확인 (테스트 데이터셋)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Y2YqfegLFobt"
   },
   "outputs": [],
   "source": [
    "# Final evaluation of the model\n",
    "scores = model.evaluate(testX, testY, verbose=0)\n",
    "print(\"Baseline Error: %.2f%%\" % (100-scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "P9pKn3KijmbK"
   },
   "source": [
    "#STEP 7: 학습된 weight 살펴보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FmTwHrdQFobw"
   },
   "outputs": [],
   "source": [
    "# Visualize weights\n",
    "W = model.layers[0].get_weights()[0]\n",
    "print(\"W shape : \", W.shape)\n",
    "\n",
    "W = np.transpose(W, (1,0))\n",
    "\n",
    "plt.figure(figsize=(15, 15), frameon=False)\n",
    "for ind, val in enumerate(W):\n",
    "    plt.subplot(5, 5, ind + 1)\n",
    "    im = val.reshape((28,28))\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(im, cmap='gray',interpolation='nearest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kuYHyuZvDS24"
   },
   "outputs": [],
   "source": [
    "model.layers[0].get_weights()[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hlxVSWn8kKXH"
   },
   "source": [
    "![대체 텍스트](https://www.pyimagesearch.com/wp-content/uploads/2019/02/fashion_mnist_obtaining.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "q-E36CT-koWF"
   },
   "source": [
    "#STEP 8: 두번째 인공지능 모델 (MLP)\n",
    "\n",
    "![대체 텍스트](https://www.researchgate.net/profile/Hadley_Brooks/publication/270274130/figure/fig3/AS:667886670594050@1536247999230/Architecture-of-a-multilayer-neural-network-with-one-hidden-layer-The-input-layer.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OCxtcC-TkjGo"
   },
   "outputs": [],
   "source": [
    "def multi_linear_perceptron_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Dense(256, input_dim=num_pixels, kernel_initializer='normal', activation='sigmoid'))\n",
    "    model.add(Dense(num_classes, kernel_initializer='normal', activation='softmax'))\n",
    "    \n",
    "    # compile model\n",
    "    sgd = optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ApdIJBsjFob1"
   },
   "outputs": [],
   "source": [
    "# build the model\n",
    "model = multi_linear_perceptron_model()\n",
    "model.summary()\n",
    "\n",
    "# fix random seed for reproductibility\n",
    "seed = 7\n",
    "np.random.seed(seed)\n",
    "\n",
    "# Fit the model\n",
    "hist = model.fit(trainX, trainY, validation_data=(valX, valY), epochs=20, batch_size=64, verbose=2)\n",
    "model.save('multi_linear_perceptron_model.h5')\n",
    "\n",
    "# 5. 학습과정 살펴보기\n",
    "fig, loss_ax = plt.subplots()\n",
    "\n",
    "acc_ax = loss_ax.twinx()\n",
    "\n",
    "loss_ax.plot(hist.history['loss'], 'y', label='train loss')\n",
    "loss_ax.plot(hist.history['val_loss'], 'r', label='val loss')\n",
    "loss_ax.set_ylim([0.0, 1.5])\n",
    "\n",
    "acc_ax.plot(hist.history['accuracy'], 'b', label='train acc')\n",
    "acc_ax.plot(hist.history['val_accuracy'], 'g', label='val acc')\n",
    "acc_ax.set_ylim([0.5, 1.0])\n",
    "\n",
    "loss_ax.set_xlabel('epoch')\n",
    "loss_ax.set_ylabel('loss')\n",
    "acc_ax.set_ylabel('accuray')\n",
    "\n",
    "loss_ax.legend(loc='upper left')\n",
    "acc_ax.legend(loc='lower left')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dSEjknxsFob4"
   },
   "outputs": [],
   "source": [
    "# Final evaluation of the model\n",
    "scores = model.evaluate(testX, testY, verbose=0)\n",
    "print(\"Error: %.2f%%\" % (100-scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eLQtffyil8WK"
   },
   "source": [
    "#STEP 9: 세번째 인공지능 모델 (DEEP-MLP)\n",
    "\n",
    "![대체 텍스트](https://i.stack.imgur.com/OH3gI.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "26LDMwE1InL2"
   },
   "source": [
    "![대체 텍스트](http://www.saedsayad.com/images/ANN_Sigmoid.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rrRqJK2KFob8"
   },
   "outputs": [],
   "source": [
    "def deep_perceptron_initial_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Dense(256, input_dim=num_pixels, kernel_initializer='normal', activation='sigmoid'))\n",
    "    model.add(Dense(256, kernel_initializer='normal', activation='sigmoid'))\n",
    "    model.add(Dense(256, kernel_initializer='normal', activation='sigmoid'))\n",
    "    model.add(Dense(256, kernel_initializer='normal', activation='sigmoid')) \n",
    "    model.add(Dense(256, kernel_initializer='normal', activation='sigmoid'))\n",
    "    model.add(Dense(num_classes, kernel_initializer='normal', activation='softmax'))\n",
    "    \n",
    "    # compile model\n",
    "    sgd = optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3SIj7p4dFob_"
   },
   "outputs": [],
   "source": [
    "# build the model\n",
    "model = deep_perceptron_initial_model()\n",
    "model.summary()\n",
    "\n",
    "# fix random seed for reproductibility\n",
    "seed = 7\n",
    "np.random.seed(seed)\n",
    "\n",
    "# Fit the model\n",
    "hist = model.fit(trainX, trainY, validation_data=(valX, valY), epochs=20, batch_size=64, verbose=2)\n",
    "model.save('deep_perceptron_initial_model.h5')\n",
    "\n",
    "# 5. 학습과정 살펴보기\n",
    "fig, loss_ax = plt.subplots()\n",
    "\n",
    "acc_ax = loss_ax.twinx()\n",
    "\n",
    "loss_ax.plot(hist.history['loss'], 'y', label='train loss')\n",
    "loss_ax.plot(hist.history['val_loss'], 'r', label='val loss')\n",
    "\n",
    "acc_ax.plot(hist.history['accuracy'], 'b', label='train acc')\n",
    "acc_ax.plot(hist.history['val_accuracy'], 'g', label='val acc')\n",
    "\n",
    "loss_ax.set_xlabel('epoch')\n",
    "loss_ax.set_ylabel('loss')\n",
    "acc_ax.set_ylabel('accuracy')\n",
    "\n",
    "loss_ax.legend(loc='upper left')\n",
    "acc_ax.legend(loc='lower left')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dTTj4GeCJ8K2"
   },
   "outputs": [],
   "source": [
    "# Final evaluation of the model\n",
    "scores = model.evaluate(testX, testY, verbose=0)\n",
    "print(\"Error: %.2f%%\" % (100-scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WYfMo7tFnBed"
   },
   "source": [
    "#STEP 10: 세번째 인공지능 모델의 문제점과 개선"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sOdXlYa0YFTZ"
   },
   "source": [
    "![대체 텍스트](https://image.slidesharecdn.com/usuconference-deeplearning-160418191119/95/introduction-to-deep-learning-7-638.jpg?cb=1461006739)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BDmw6R8_GNed"
   },
   "source": [
    "![대체 텍스트](https://smartstuartkim.files.wordpress.com/2019/02/vanishinggradient-1.png?w=1140&h=492)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LGPy6P2zFocH"
   },
   "outputs": [],
   "source": [
    "#  Hint\n",
    "# 'relu'\n",
    "\n",
    "def deep_perceptron_model_with_relu():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Dense(256, input_dim=num_pixels, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(256, kernel_initializer='normal', activation='????'))\n",
    "    model.add(Dense(256, kernel_initializer='normal', activation='????'))\n",
    "    model.add(Dense(256, kernel_initializer='normal', activation='????'))\n",
    "    model.add(Dense(256, kernel_initializer='normal', activation='????'))    \n",
    "    model.add(Dense(num_classes, kernel_initializer='normal', activation='softmax'))\n",
    "    # compile model\n",
    "    \n",
    "    sgd = optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uVgGp42FFocM"
   },
   "outputs": [],
   "source": [
    "# build the model\n",
    "model = deep_perceptron_model_with_relu()\n",
    "model.summary()\n",
    "\n",
    "# fix random seed for reproductibility\n",
    "seed = 7\n",
    "np.random.seed(seed)\n",
    "\n",
    "# Fit the model\n",
    "hist = model.fit(trainX, trainY, validation_data=(valX, valY), epochs=20, batch_size=64, verbose=2)\n",
    "model.save('deep_perceptron_model_with_dropout.h5')\n",
    "\n",
    "# 5. 학습과정 살펴보기\n",
    "fig, loss_ax = plt.subplots()\n",
    "\n",
    "acc_ax = loss_ax.twinx()\n",
    "\n",
    "loss_ax.plot(hist.history['loss'], 'y', label='train loss')\n",
    "loss_ax.plot(hist.history['val_loss'], 'r', label='val loss')\n",
    "loss_ax.set_ylim([0.0, 1.5])\n",
    "\n",
    "acc_ax.plot(hist.history['accuracy'], 'b', label='train acc')\n",
    "acc_ax.plot(hist.history['val_accuracy'], 'g', label='val acc')\n",
    "acc_ax.set_ylim([0.5, 1.0])\n",
    "\n",
    "loss_ax.set_xlabel('epoch')\n",
    "loss_ax.set_ylabel('loss')\n",
    "acc_ax.set_ylabel('accuracy')\n",
    "\n",
    "loss_ax.legend(loc='upper left')\n",
    "acc_ax.legend(loc='lower left')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cC6n5edlFocR"
   },
   "outputs": [],
   "source": [
    "# Final evaluation of the model\n",
    "scores = model.evaluate(testX, testY, verbose=0)\n",
    "print(\"Perceptron model with relu error: %.2f%%\" % (100-scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LLTnR7uKJm5m"
   },
   "source": [
    "![대체 텍스트](https://miro.medium.com/max/1200/1*iWQzxhVlvadk6VAJjsgXgg.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SWmvv-noJuQ-"
   },
   "outputs": [],
   "source": [
    "#  Hint\n",
    "from keras.layers import Dropout\n",
    "\n",
    "def deep_perceptron_model_with_relu_dropout():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(256, input_dim=num_pixels, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(????(0.2))\n",
    "    \n",
    "    model.add(Dense(256, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(????(0.2))\n",
    "    \n",
    "    model.add(Dense(256, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(????(0.2))\n",
    "    \n",
    "    model.add(Dense(256, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(????(0.2))\n",
    "    \n",
    "    model.add(Dense(256, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(????(0.2))\n",
    "    \n",
    "    model.add(Dense(num_classes, kernel_initializer='normal', activation='softmax'))\n",
    "    \n",
    "    # compile model\n",
    "    sgd = optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "58BeovzhJ59T"
   },
   "outputs": [],
   "source": [
    "# build the model\n",
    "model = deep_perceptron_model_with_relu_dropout()\n",
    "model.summary()\n",
    "\n",
    "# fix random seed for reproductibility\n",
    "seed = 7\n",
    "np.random.seed(seed)\n",
    "\n",
    "# Fit the model\n",
    "hist = model.fit(trainX, trainY, validation_data=(valX, valY), epochs=20, batch_size=64, verbose=2)\n",
    "model.save('deep_perceptron_model_with_dropout.h5')\n",
    "\n",
    "# 5. 학습과정 살펴보기\n",
    "fig, loss_ax = plt.subplots()\n",
    "\n",
    "acc_ax = loss_ax.twinx()\n",
    "\n",
    "loss_ax.plot(hist.history['loss'], 'y', label='train loss')\n",
    "loss_ax.plot(hist.history['val_loss'], 'r', label='val loss')\n",
    "loss_ax.set_ylim([0.0, 1.5])\n",
    "\n",
    "acc_ax.plot(hist.history['acc'], 'b', label='train acc')\n",
    "acc_ax.plot(hist.history['val_acc'], 'g', label='val acc')\n",
    "acc_ax.set_ylim([0.5, 1.0])\n",
    "\n",
    "loss_ax.set_xlabel('epoch')\n",
    "loss_ax.set_ylabel('loss')\n",
    "acc_ax.set_ylabel('accuray')\n",
    "\n",
    "loss_ax.legend(loc='upper left')\n",
    "acc_ax.legend(loc='lower left')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6cIJuuuENAPP"
   },
   "outputs": [],
   "source": [
    "# Final evaluation of the model\n",
    "scores = model.evaluate(testX, testY, verbose=0)\n",
    "print(\"Perceptron model with relu and dropout error: %.2f%%\" % (100-scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "E955lLUboIsU"
   },
   "source": [
    "#STEP 11: 네번째 인공지능 모델 (CNN)\n",
    "\n",
    "![대체 텍스트](https://www.mdpi.com/entropy/entropy-19-00242/article_deploy/html/images/entropy-19-00242-g001.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CybrZ58MpVGW"
   },
   "source": [
    "\n",
    "# 중요! 입력데이터의 형태가 바뀌어야 한다. \n",
    "# 784 (1D) -> 28x28 (2D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "R106wlxMFocV"
   },
   "outputs": [],
   "source": [
    "# reshape to be [samples][pixels][width][height]\n",
    "trainX = trainX.reshape(50000, 28, 28, 1)\n",
    "valX = valX.reshape(10000, 28, 28, 1)\n",
    "testX = testX.reshape(10000, 28, 28, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "X2xGLQt0FocX"
   },
   "outputs": [],
   "source": [
    "def simple_cnn_model():\n",
    "    # create model    \n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Conv2D(32, (5,5), input_shape=(28, 28, 1), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    \n",
    "    # Compile model\n",
    "    sgd = optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7Y1YC0c-Foca"
   },
   "outputs": [],
   "source": [
    "# build the model\n",
    "model = simple_cnn_model()\n",
    "model.summary()\n",
    "\n",
    "# fix random seed for reproductibility\n",
    "seed = 7\n",
    "np.random.seed(seed)\n",
    "\n",
    "# Fit the model\n",
    "hist = model.fit(trainX, trainY, validation_data=(valX, valY), epochs=20, batch_size=64, verbose=2)\n",
    "model.save('simple_cnn_model.h5')\n",
    "\n",
    "# 5. 학습과정 살펴보기\n",
    "fig, loss_ax = plt.subplots()\n",
    "\n",
    "acc_ax = loss_ax.twinx()\n",
    "\n",
    "loss_ax.plot(hist.history['loss'], 'y', label='train loss')\n",
    "loss_ax.plot(hist.history['val_loss'], 'r', label='val loss')\n",
    "loss_ax.set_ylim([0.0, 1.5])\n",
    "\n",
    "acc_ax.plot(hist.history['accuracy'], 'b', label='train acc')\n",
    "acc_ax.plot(hist.history['val_accuracy'], 'g', label='val acc')\n",
    "acc_ax.set_ylim([0.5, 1.0])\n",
    "\n",
    "loss_ax.set_xlabel('epoch')\n",
    "loss_ax.set_ylabel('loss')\n",
    "acc_ax.set_ylabel('accuray')\n",
    "\n",
    "loss_ax.legend(loc='upper left')\n",
    "acc_ax.legend(loc='lower left')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ttpPwLW4Focd"
   },
   "outputs": [],
   "source": [
    "# Final evaluation of the model\n",
    "scores = model.evaluate(testX, testY, verbose=0)\n",
    "print(\"2D simple CNN error: %.2f%%\" % (100-scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-ae0K65Z1IQi"
   },
   "source": [
    "#STEP 12: Convolution kernel 살펴보기 (5x5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Uf5fPXX_Focf"
   },
   "outputs": [],
   "source": [
    "W1 = model.layers[0].get_weights()[0]\n",
    "W1 = np.squeeze(W1)\n",
    "\n",
    "print(W1.shape)\n",
    "W1 = np.transpose(W1, (2,0,1))\n",
    "\n",
    "plt.figure(figsize=(15, 15), frameon=False)\n",
    "for ind, val in enumerate(W1):\n",
    "    plt.subplot(6, 6, ind + 1)\n",
    "    im = val.reshape((5,5))\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(im, cmap='gray',interpolation='nearest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Tdy-keR_Foch"
   },
   "outputs": [],
   "source": [
    "convout1_f = K.function([model.layers[0].input], [model.layers[1].output])\n",
    "\n",
    "x_rep = convout1_f([testX[0:3]])\n",
    "x_rep = np.squeeze(x_rep)\n",
    "\n",
    "print(x_rep.shape)\n",
    "\n",
    "for this_x_rep in x_rep:\n",
    "    plt.figure(figsize=(15, 15), frameon=False)\n",
    "    \n",
    "    for i in range (this_x_rep.shape[2]):\n",
    "        val = this_x_rep[:,:,i]\n",
    "        plt.subplot(6, 6, i + 1)\n",
    "        plt.axis(\"off\")\n",
    "        plt.imshow(val, cmap='gray',interpolation='nearest')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OhErkuAD1laZ"
   },
   "source": [
    "#STEP 13: 마지막 인공지능 모델 (VGG-like CNN)\n",
    "\n",
    "![대체 텍스트](https://neurohive.io/wp-content/uploads/2018/11/vgg16-1-e1542731207177.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zBIhP_xVFock"
   },
   "outputs": [],
   "source": [
    "def cnn_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Conv2D(32, (3,3), input_shape=(28, 28, 1)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation(activation='relu'))\n",
    "    \n",
    "    model.add(Conv2D(32, (3,3)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation(activation='relu'))\n",
    "    \n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    \n",
    "    model.add(Conv2D(64, (3,3)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation(activation='relu'))\n",
    "    \n",
    "    model.add(Conv2D(64, (3,3)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation(activation='relu'))\n",
    "    \n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    \n",
    "    # Compile model\n",
    "    sgd = optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3OB5CedfFocl"
   },
   "outputs": [],
   "source": [
    "# build the model\n",
    "model = cnn_model()\n",
    "\n",
    "# fix random seed for reproductibility\n",
    "seed = 7\n",
    "np.random.seed(seed)\n",
    "\n",
    "# Fit the model\n",
    "hist = model.fit(trainX, trainY, validation_data=(valX, valY), epochs=20, batch_size=64, verbose=2)\n",
    "model.save('cnn_model.h5')\n",
    "\n",
    "# 5. 학습과정 살펴보기\n",
    "fig, loss_ax = plt.subplots()\n",
    "\n",
    "acc_ax = loss_ax.twinx()\n",
    "\n",
    "loss_ax.plot(hist.history['loss'], 'y', label='train loss')\n",
    "loss_ax.plot(hist.history['val_loss'], 'r', label='val loss')\n",
    "loss_ax.set_ylim([0.0, 1.5])\n",
    "\n",
    "acc_ax.plot(hist.history['accuracy'], 'b', label='train acc')\n",
    "acc_ax.plot(hist.history['val_accuracy'], 'g', label='val acc')\n",
    "acc_ax.set_ylim([0.5, 1.0])\n",
    "\n",
    "loss_ax.set_xlabel('epoch')\n",
    "loss_ax.set_ylabel('loss')\n",
    "acc_ax.set_ylabel('accuray')\n",
    "\n",
    "loss_ax.legend(loc='upper left')\n",
    "acc_ax.legend(loc='lower left')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FR3ZmGKkFocn"
   },
   "outputs": [],
   "source": [
    "# Final evaluation of the model\n",
    "scores = model.evaluate(testX, testY, verbose=0)\n",
    "print(\"VGG-like CNN error: %.2f%%\" % (100-scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BM17JvSG1_y_"
   },
   "source": [
    "#STEP 14: 결과 확인하기 (틀린 것 들만)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yMnR251GFocp"
   },
   "outputs": [],
   "source": [
    "# 7. 모델 사용하기\n",
    "yhat_test = model.predict(testX, batch_size=32)\n",
    "\n",
    "plt_row = 5\n",
    "plt_col = 5\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (20,20)\n",
    "\n",
    "f, axarr = plt.subplots(plt_row, plt_col)\n",
    "\n",
    "cnt = 0\n",
    "i = 0\n",
    "\n",
    "while cnt < (plt_row*plt_col):\n",
    "    \n",
    "    if np.argmax(testY[i]) == np.argmax(yhat_test[i]):\n",
    "        i += 1\n",
    "        continue\n",
    "    \n",
    "    sub_plt = axarr[(int)(cnt/plt_row), cnt%plt_col]\n",
    "    sub_plt.axis('off')\n",
    "    sub_plt.imshow(testX[i].reshape(width, height), cmap='gray')\n",
    "    sub_plt_title = 'R: ' + labelNames[np.argmax(testY[i])] + '(%.2f)'% (yhat_test[i][np.argmax(testY[i])]) + ' P: ' + labelNames[np.argmax(yhat_test[i])] + '(%.2f)'% (  yhat_test[i][np.argmax(yhat_test[i])])\n",
    "    sub_plt.set_title(sub_plt_title)\n",
    "\n",
    "    i += 1    \n",
    "    cnt += 1\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Fashion-MNIST_tutorial_with_Keras.ipynb의 사본의 사본의 사본",
   "provenance": [
    {
     "file_id": "https://github.com/mi2rl/ai_handson/blob/master/KoSAIM_Summer_School_2019_Handson_01_Fashion_MNIST_Tutorial_with_Keras.ipynb",
     "timestamp": 1566791743000
    },
    {
     "file_id": "https://github.com/mi2rl/ai_handson/blob/master/KoSAIM_Summer_School_2019_Handson_01_Fashion_MNIST_Tutorial_with_Keras.ipynb",
     "timestamp": 1566526324457
    },
    {
     "file_id": "https://github.com/mi2rl/ai_handson/blob/master/KoSAIM_Summer_School_2019_Handson_01_Fashion_MNIST_Tutorial.ipynb",
     "timestamp": 1566476677527
    }
   ],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
