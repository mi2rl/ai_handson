{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Fashion-MNIST_tutorial_with_Keras.ipynb의 사본의 사본의 사본",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ysPmtr34QZI5"
      },
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WsAx4toWxKZE"
      },
      "source": [
        "![대체 텍스트](https://www.pyimagesearch.com/wp-content/uploads/2019/02/fashion_mnist_dataset_sample.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xhfmxR8o2sWd"
      },
      "source": [
        "\n",
        "# MNIST is too easy.\n",
        "Convolutional nets can achieve 99.7% on MNIST. Classic machine learning algorithms can also achieve 97% easily. \n",
        "# MNIST is overused. \n",
        "In this April 2017 Twitter thread, Google Brain research scientist and deep learning expert Ian Goodfellow calls for people to move away from MNIST.\n",
        "# MNIST can not represent modern CV tasks.\n",
        "\n",
        "# Fashion MNIST dataset\n",
        "Similar to the MNIST digit dataset, the Fashion MNIST dataset includes:\n",
        "\n",
        "60,000 training examples\n",
        "\n",
        "10,000 testing examples\n",
        "\n",
        "10 classes\n",
        "\n",
        "28×28 grayscale/single channel images"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sFD7RLgA3ouu"
      },
      "source": [
        "![대체 텍스트](https://www.pyimagesearch.com/wp-content/uploads/2019/02/fashion_mnist_obtaining.jpg)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OFK9236vMmZO"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten, Activation\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
        "from tensorflow.keras.layers import Dropout, BatchNormalization\n",
        "\n",
        "from tensorflow.keras import optimizers\n",
        "\n",
        "from tensorflow.keras import backend as K\n",
        "\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "emDMGh2lWBHd"
      },
      "source": [
        "#STEP 1: Fashion MNIST 데이터 읽어들이기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1gjzm6goU1rp"
      },
      "source": [
        "from tensorflow.keras.datasets import fashion_mnist\n",
        "((trainX, trainY), (testX, testY)) = fashion_mnist.load_data()\n",
        "\n",
        "# initialize the label names\n",
        "labelNames = [\"top\", \"trouser\", \"pullover\", \"dress\", \"coat\", \n",
        "             \"sandal\", \"shirt\", \"sneaker\", \"bag\", \"ankle boot\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8RwCuia7X_gD"
      },
      "source": [
        "#STEP 2: 데이터 살펴보기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YFgKtGe4YFoT"
      },
      "source": [
        "plt_row = 5\n",
        "plt_col = 5\n",
        "\n",
        "width = height = 28\n",
        "\n",
        "plt.rcParams[\"figure.figsize\"] = (15,15)\n",
        "\n",
        "f, axarr = plt.subplots(plt_row, plt_col)\n",
        "\n",
        "for i in range(plt_row*plt_col):\n",
        "\n",
        "    sub_plt = axarr[int(i/plt_row), i%plt_col]\n",
        "    sub_plt.axis('off')\n",
        "    sub_plt.imshow(testX[i].reshape(width, height), cmap='gray')\n",
        "    sub_plt_title = 'R: ' + labelNames[testY[i]]\n",
        "    sub_plt.set_title(sub_plt_title)\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aPIIgjLbc9Cu"
      },
      "source": [
        "#STEP 3: 딥러닝을 위한 데이터 전처리"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q9Lx0-DQdD93"
      },
      "source": [
        "# flatten 28*28 images to a 784 vector for each image\n",
        "width = height = 28\n",
        "num_pixels = width * height\n",
        "trainX = trainX.reshape(60000, num_pixels).astype('float32') / 255.0\n",
        "testX = testX.reshape(10000, num_pixels).astype('float32') / 255.0\n",
        "\n",
        "# 훈련셋과 검증셋 분리\n",
        "valX = trainX[50000:]\n",
        "valY = trainY[50000:]\n",
        "trainX = trainX[:50000]\n",
        "trainY = trainY[:50000]\n",
        "\n",
        "# one hot encode outputs\n",
        "num_classes = 10\n",
        "trainY = tf.keras.utils.to_categorical(trainY, num_classes)\n",
        "valY = tf.keras.utils.to_categorical(valY, num_classes)\n",
        "testY = tf.keras.utils.to_categorical(testY, num_classes)\n",
        "\n",
        "print ('train shape: \\t', trainX.shape)\n",
        "print ('valid shape: \\t', valX.shape)\n",
        "print ('test shape: \\t', testX.shape)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wtemFso_KR6o"
      },
      "source": [
        "trainX.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6BhD2uuPfgZG"
      },
      "source": [
        "#STEP 4: 첫번째 인공지능 모델 (퍼셉트론)\n",
        "\n",
        "![대체 텍스트](https://www.simplilearn.com/ice9/free_resources_article_thumb/diagram-of-a-biological-neuron.jpg)\n",
        "\n",
        "![대체 텍스트](http://bit.ly/2ldH0Bg)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H-P5NCZHfxNs"
      },
      "source": [
        "def logistic_regression_model():\n",
        "    # create model\n",
        "    model = Sequential()\n",
        "    \n",
        "    model.add(Dense(num_classes, input_dim=num_pixels, kernel_initializer='normal', activation='softmax'))\n",
        "    \n",
        "    # compile model\n",
        "    sgd = optimizers.SGD(learning_rate=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
        "    model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
        "    \n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DVwdrbt2gyWg"
      },
      "source": [
        "#STEP 5: 첫번째 인공지능 모델 학습!!!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gCgT7q9dgx6C"
      },
      "source": [
        "# build the model\n",
        "model = logistic_regression_model()\n",
        "model.summary()\n",
        "# fix random seed for reproductibility\n",
        "seed = 7\n",
        "np.random.seed(seed)\n",
        "\n",
        "# FIT THE MODEL - OPTIMIZATION\n",
        "hist = model.fit(trainX, trainY, validation_data=(valX, valY), epochs=20, batch_size=64, verbose=2)\n",
        "model.save('logistic_regression_model.h5')\n",
        "\n",
        "# 학습과정 살펴보기\n",
        "fig, loss_ax = plt.subplots()\n",
        "\n",
        "acc_ax = loss_ax.twinx()\n",
        "\n",
        "loss_ax.plot(hist.history['loss'], 'y', label='train loss')\n",
        "loss_ax.plot(hist.history['val_loss'], 'r', label='val loss')\n",
        "loss_ax.set_ylim([0.0, 1.5])\n",
        "\n",
        "acc_ax.plot(hist.history['accuracy'], 'b', label='train acc')\n",
        "acc_ax.plot(hist.history['val_accuracy'], 'g', label='val acc')\n",
        "acc_ax.set_ylim([0.5, 1.0])\n",
        "\n",
        "loss_ax.set_xlabel('epoch')\n",
        "loss_ax.set_ylabel('loss')\n",
        "acc_ax.set_ylabel('accuray')\n",
        "\n",
        "loss_ax.legend(loc='upper left')\n",
        "acc_ax.legend(loc='lower left')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J_i3PjlRjRSY"
      },
      "source": [
        "#STEP 6: 결과 확인 (테스트 데이터셋)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y2YqfegLFobt"
      },
      "source": [
        "# Final evaluation of the model\n",
        "scores = model.evaluate(testX, testY, verbose=0)\n",
        "print(\"Baseline Error: %.2f%%\" % (100-scores[1]*100))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P9pKn3KijmbK"
      },
      "source": [
        "#STEP 7: 학습된 weight 살펴보기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FmTwHrdQFobw"
      },
      "source": [
        "# Visualize weights\n",
        "W = model.layers[0].get_weights()[0]\n",
        "print(\"W shape : \", W.shape)\n",
        "\n",
        "W = np.transpose(W, (1,0))\n",
        "\n",
        "plt.figure(figsize=(15, 15), frameon=False)\n",
        "for ind, val in enumerate(W):\n",
        "    plt.subplot(5, 5, ind + 1)\n",
        "    im = val.reshape((28,28))\n",
        "    plt.axis(\"off\")\n",
        "    plt.imshow(im, cmap='gray',interpolation='nearest')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kuYHyuZvDS24"
      },
      "source": [
        "model.layers[0].get_weights()[0].shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hlxVSWn8kKXH"
      },
      "source": [
        "![대체 텍스트](https://www.pyimagesearch.com/wp-content/uploads/2019/02/fashion_mnist_obtaining.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q-E36CT-koWF"
      },
      "source": [
        "#STEP 8: 두번째 인공지능 모델 (MLP)\n",
        "\n",
        "![대체 텍스트](https://www.researchgate.net/profile/Hadley_Brooks/publication/270274130/figure/fig3/AS:667886670594050@1536247999230/Architecture-of-a-multilayer-neural-network-with-one-hidden-layer-The-input-layer.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OCxtcC-TkjGo"
      },
      "source": [
        "def multi_linear_perceptron_model():\n",
        "    # create model\n",
        "    model = Sequential()\n",
        "    \n",
        "    model.add(Dense(256, input_dim=num_pixels, kernel_initializer='normal', activation='sigmoid'))\n",
        "    model.add(Dense(num_classes, kernel_initializer='normal', activation='softmax'))\n",
        "    \n",
        "    # compile model\n",
        "    sgd = optimizers.SGD(learning_rate=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
        "    model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
        "    \n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ApdIJBsjFob1"
      },
      "source": [
        "# build the model\n",
        "model = multi_linear_perceptron_model()\n",
        "model.summary()\n",
        "\n",
        "# fix random seed for reproductibility\n",
        "seed = 7\n",
        "np.random.seed(seed)\n",
        "\n",
        "# Fit the model\n",
        "hist = model.fit(trainX, trainY, validation_data=(valX, valY), epochs=20, batch_size=64, verbose=2)\n",
        "model.save('multi_linear_perceptron_model.h5')\n",
        "\n",
        "# 5. 학습과정 살펴보기\n",
        "fig, loss_ax = plt.subplots()\n",
        "\n",
        "acc_ax = loss_ax.twinx()\n",
        "\n",
        "loss_ax.plot(hist.history['loss'], 'y', label='train loss')\n",
        "loss_ax.plot(hist.history['val_loss'], 'r', label='val loss')\n",
        "loss_ax.set_ylim([0.0, 1.5])\n",
        "\n",
        "acc_ax.plot(hist.history['accuracy'], 'b', label='train acc')\n",
        "acc_ax.plot(hist.history['val_accuracy'], 'g', label='val acc')\n",
        "acc_ax.set_ylim([0.5, 1.0])\n",
        "\n",
        "loss_ax.set_xlabel('epoch')\n",
        "loss_ax.set_ylabel('loss')\n",
        "acc_ax.set_ylabel('accuray')\n",
        "\n",
        "loss_ax.legend(loc='upper left')\n",
        "acc_ax.legend(loc='lower left')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dSEjknxsFob4"
      },
      "source": [
        "# Final evaluation of the model\n",
        "scores = model.evaluate(testX, testY, verbose=0)\n",
        "print(\"Error: %.2f%%\" % (100-scores[1]*100))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eLQtffyil8WK"
      },
      "source": [
        "#STEP 9: 세번째 인공지능 모델 (DEEP-MLP)\n",
        "\n",
        "![대체 텍스트](https://i.stack.imgur.com/OH3gI.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "26LDMwE1InL2"
      },
      "source": [
        "![대체 텍스트](http://www.saedsayad.com/images/ANN_Sigmoid.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rrRqJK2KFob8"
      },
      "source": [
        "def deep_perceptron_initial_model():\n",
        "    # create model\n",
        "    model = Sequential()\n",
        "    \n",
        "    model.add(Dense(256, input_dim=num_pixels, kernel_initializer='normal', activation='sigmoid'))\n",
        "    model.add(Dense(256, kernel_initializer='normal', activation='sigmoid'))\n",
        "    model.add(Dense(256, kernel_initializer='normal', activation='sigmoid'))\n",
        "    model.add(Dense(256, kernel_initializer='normal', activation='sigmoid')) \n",
        "    model.add(Dense(256, kernel_initializer='normal', activation='sigmoid'))\n",
        "    model.add(Dense(num_classes, kernel_initializer='normal', activation='softmax'))\n",
        "    \n",
        "    # compile model\n",
        "    sgd = optimizers.SGD(learning_rate=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
        "    model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
        "    \n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3SIj7p4dFob_"
      },
      "source": [
        "# build the model\n",
        "model = deep_perceptron_initial_model()\n",
        "model.summary()\n",
        "\n",
        "# fix random seed for reproductibility\n",
        "seed = 7\n",
        "np.random.seed(seed)\n",
        "\n",
        "# Fit the model\n",
        "hist = model.fit(trainX, trainY, validation_data=(valX, valY), epochs=20, batch_size=64, verbose=2)\n",
        "model.save('deep_perceptron_initial_model.h5')\n",
        "\n",
        "# 5. 학습과정 살펴보기\n",
        "fig, loss_ax = plt.subplots()\n",
        "\n",
        "acc_ax = loss_ax.twinx()\n",
        "\n",
        "loss_ax.plot(hist.history['loss'], 'y', label='train loss')\n",
        "loss_ax.plot(hist.history['val_loss'], 'r', label='val loss')\n",
        "\n",
        "acc_ax.plot(hist.history['accuracy'], 'b', label='train acc')\n",
        "acc_ax.plot(hist.history['val_accuracy'], 'g', label='val acc')\n",
        "\n",
        "loss_ax.set_xlabel('epoch')\n",
        "loss_ax.set_ylabel('loss')\n",
        "acc_ax.set_ylabel('accuracy')\n",
        "\n",
        "loss_ax.legend(loc='upper left')\n",
        "acc_ax.legend(loc='lower left')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dTTj4GeCJ8K2"
      },
      "source": [
        "# Final evaluation of the model\n",
        "scores = model.evaluate(testX, testY, verbose=0)\n",
        "print(\"Error: %.2f%%\" % (100-scores[1]*100))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WYfMo7tFnBed"
      },
      "source": [
        "#STEP 10: 세번째 인공지능 모델의 문제점과 개선"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sOdXlYa0YFTZ"
      },
      "source": [
        "![대체 텍스트](https://image.slidesharecdn.com/usuconference-deeplearning-160418191119/95/introduction-to-deep-learning-7-638.jpg?cb=1461006739)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BDmw6R8_GNed"
      },
      "source": [
        "![대체 텍스트](https://smartstuartkim.files.wordpress.com/2019/02/vanishinggradient-1.png?w=1140&h=492)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LGPy6P2zFocH"
      },
      "source": [
        "#  Hint\n",
        "# 'relu'\n",
        "\n",
        "def deep_perceptron_model_with_relu():\n",
        "    # create model\n",
        "    model = Sequential()\n",
        "    \n",
        "    model.add(Dense(256, input_dim=num_pixels, kernel_initializer='normal', activation='relu'))\n",
        "    model.add(Dense(256, kernel_initializer='normal', activation='????'))\n",
        "    model.add(Dense(256, kernel_initializer='normal', activation='????'))\n",
        "    model.add(Dense(256, kernel_initializer='normal', activation='????'))\n",
        "    model.add(Dense(256, kernel_initializer='normal', activation='????'))    \n",
        "    model.add(Dense(num_classes, kernel_initializer='normal', activation='softmax'))\n",
        "    # compile model\n",
        "    \n",
        "    sgd = optimizers.SGD(learning_rate=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
        "    model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
        "    \n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uVgGp42FFocM"
      },
      "source": [
        "# build the model\n",
        "model = deep_perceptron_model_with_relu()\n",
        "model.summary()\n",
        "\n",
        "# fix random seed for reproductibility\n",
        "seed = 7\n",
        "np.random.seed(seed)\n",
        "\n",
        "# Fit the model\n",
        "hist = model.fit(trainX, trainY, validation_data=(valX, valY), epochs=20, batch_size=64, verbose=2)\n",
        "model.save('deep_perceptron_model_with_dropout.h5')\n",
        "\n",
        "# 5. 학습과정 살펴보기\n",
        "fig, loss_ax = plt.subplots()\n",
        "\n",
        "acc_ax = loss_ax.twinx()\n",
        "\n",
        "loss_ax.plot(hist.history['loss'], 'y', label='train loss')\n",
        "loss_ax.plot(hist.history['val_loss'], 'r', label='val loss')\n",
        "loss_ax.set_ylim([0.0, 1.5])\n",
        "\n",
        "acc_ax.plot(hist.history['accuracy'], 'b', label='train acc')\n",
        "acc_ax.plot(hist.history['val_accuracy'], 'g', label='val acc')\n",
        "acc_ax.set_ylim([0.5, 1.0])\n",
        "\n",
        "loss_ax.set_xlabel('epoch')\n",
        "loss_ax.set_ylabel('loss')\n",
        "acc_ax.set_ylabel('accuracy')\n",
        "\n",
        "loss_ax.legend(loc='upper left')\n",
        "acc_ax.legend(loc='lower left')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cC6n5edlFocR"
      },
      "source": [
        "# Final evaluation of the model\n",
        "scores = model.evaluate(testX, testY, verbose=0)\n",
        "print(\"Perceptron model with relu error: %.2f%%\" % (100-scores[1]*100))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LLTnR7uKJm5m"
      },
      "source": [
        "![대체 텍스트](https://miro.medium.com/max/1200/1*iWQzxhVlvadk6VAJjsgXgg.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SWmvv-noJuQ-"
      },
      "source": [
        "#  Hint\n",
        "# 'Dropout'\n",
        "\n",
        "def deep_perceptron_model_with_relu_dropout():\n",
        "    # create model\n",
        "    model = Sequential()\n",
        "    model.add(Dense(256, input_dim=num_pixels, kernel_initializer='normal', activation='relu'))\n",
        "    model.add(????(0.2))\n",
        "    \n",
        "    model.add(Dense(256, kernel_initializer='normal', activation='relu'))\n",
        "    model.add(????(0.2))\n",
        "    \n",
        "    model.add(Dense(256, kernel_initializer='normal', activation='relu'))\n",
        "    model.add(????(0.2))\n",
        "    \n",
        "    model.add(Dense(256, kernel_initializer='normal', activation='relu'))\n",
        "    model.add(????(0.2))\n",
        "    \n",
        "    model.add(Dense(256, kernel_initializer='normal', activation='relu'))\n",
        "    model.add(????(0.2))\n",
        "    \n",
        "    model.add(Dense(num_classes, kernel_initializer='normal', activation='softmax'))\n",
        "    \n",
        "    # compile model\n",
        "    sgd = optimizers.SGD(learning_rate=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
        "    model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
        "    \n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "58BeovzhJ59T"
      },
      "source": [
        "# build the model\n",
        "model = deep_perceptron_model_with_relu_dropout()\n",
        "model.summary()\n",
        "\n",
        "# fix random seed for reproductibility\n",
        "seed = 7\n",
        "np.random.seed(seed)\n",
        "\n",
        "# Fit the model\n",
        "hist = model.fit(trainX, trainY, validation_data=(valX, valY), epochs=20, batch_size=64, verbose=2)\n",
        "model.save('deep_perceptron_model_with_dropout.h5')\n",
        "\n",
        "# 5. 학습과정 살펴보기\n",
        "fig, loss_ax = plt.subplots()\n",
        "\n",
        "acc_ax = loss_ax.twinx()\n",
        "\n",
        "loss_ax.plot(hist.history['loss'], 'y', label='train loss')\n",
        "loss_ax.plot(hist.history['val_loss'], 'r', label='val loss')\n",
        "loss_ax.set_ylim([0.0, 1.5])\n",
        "\n",
        "acc_ax.plot(hist.history['acc'], 'b', label='train acc')\n",
        "acc_ax.plot(hist.history['val_acc'], 'g', label='val acc')\n",
        "acc_ax.set_ylim([0.5, 1.0])\n",
        "\n",
        "loss_ax.set_xlabel('epoch')\n",
        "loss_ax.set_ylabel('loss')\n",
        "acc_ax.set_ylabel('accuray')\n",
        "\n",
        "loss_ax.legend(loc='upper left')\n",
        "acc_ax.legend(loc='lower left')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6cIJuuuENAPP"
      },
      "source": [
        "# Final evaluation of the model\n",
        "scores = model.evaluate(testX, testY, verbose=0)\n",
        "print(\"Perceptron model with relu and dropout error: %.2f%%\" % (100-scores[1]*100))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E955lLUboIsU"
      },
      "source": [
        "#STEP 11: 네번째 인공지능 모델 (CNN)\n",
        "\n",
        "![대체 텍스트](https://www.mdpi.com/entropy/entropy-19-00242/article_deploy/html/images/entropy-19-00242-g001.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CybrZ58MpVGW"
      },
      "source": [
        "\n",
        "# 중요! 입력데이터의 형태가 바뀌어야 한다. \n",
        "# 784 (1D) -> 28x28 (2D)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R106wlxMFocV"
      },
      "source": [
        "# reshape to be [samples][pixels][width][height]\n",
        "trainX = trainX.reshape(50000, 28, 28, 1)\n",
        "valX = valX.reshape(10000, 28, 28, 1)\n",
        "testX = testX.reshape(10000, 28, 28, 1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X2xGLQt0FocX"
      },
      "source": [
        "def simple_cnn_model():\n",
        "    # create model    \n",
        "    model = Sequential()\n",
        "    \n",
        "    model.add(Conv2D(32, (5,5), input_shape=(28, 28, 1), activation='relu'))\n",
        "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "    \n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(128, activation='relu'))\n",
        "    model.add(Dense(num_classes, activation='softmax'))\n",
        "    \n",
        "    # Compile model\n",
        "    sgd = optimizers.SGD(learning_rate=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
        "    model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
        "    \n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Y1YC0c-Foca"
      },
      "source": [
        "# build the model\n",
        "model = simple_cnn_model()\n",
        "model.summary()\n",
        "\n",
        "# fix random seed for reproductibility\n",
        "seed = 7\n",
        "np.random.seed(seed)\n",
        "\n",
        "# Fit the model\n",
        "hist = model.fit(trainX, trainY, validation_data=(valX, valY), epochs=20, batch_size=64, verbose=2)\n",
        "model.save('simple_cnn_model.h5')\n",
        "\n",
        "# 5. 학습과정 살펴보기\n",
        "fig, loss_ax = plt.subplots()\n",
        "\n",
        "acc_ax = loss_ax.twinx()\n",
        "\n",
        "loss_ax.plot(hist.history['loss'], 'y', label='train loss')\n",
        "loss_ax.plot(hist.history['val_loss'], 'r', label='val loss')\n",
        "loss_ax.set_ylim([0.0, 1.5])\n",
        "\n",
        "acc_ax.plot(hist.history['accuracy'], 'b', label='train acc')\n",
        "acc_ax.plot(hist.history['val_accuracy'], 'g', label='val acc')\n",
        "acc_ax.set_ylim([0.5, 1.0])\n",
        "\n",
        "loss_ax.set_xlabel('epoch')\n",
        "loss_ax.set_ylabel('loss')\n",
        "acc_ax.set_ylabel('accuray')\n",
        "\n",
        "loss_ax.legend(loc='upper left')\n",
        "acc_ax.legend(loc='lower left')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ttpPwLW4Focd"
      },
      "source": [
        "# Final evaluation of the model\n",
        "scores = model.evaluate(testX, testY, verbose=0)\n",
        "print(\"2D simple CNN error: %.2f%%\" % (100-scores[1]*100))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ae0K65Z1IQi"
      },
      "source": [
        "#STEP 12: Convolution kernel 살펴보기 (5x5)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uf5fPXX_Focf"
      },
      "source": [
        "W1 = model.layers[0].get_weights()[0]\n",
        "W1 = np.squeeze(W1)\n",
        "\n",
        "print(W1.shape)\n",
        "W1 = np.transpose(W1, (2,0,1))\n",
        "\n",
        "plt.figure(figsize=(15, 15), frameon=False)\n",
        "for ind, val in enumerate(W1):\n",
        "    plt.subplot(6, 6, ind + 1)\n",
        "    im = val.reshape((5,5))\n",
        "    plt.axis(\"off\")\n",
        "    plt.imshow(im, cmap='gray',interpolation='nearest')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tdy-keR_Foch"
      },
      "source": [
        "convout1_f = K.function([model.layers[0].input], [model.layers[1].output])\n",
        "\n",
        "x_rep = convout1_f([testX[0:3]])\n",
        "x_rep = np.squeeze(x_rep)\n",
        "\n",
        "print(x_rep.shape)\n",
        "\n",
        "for this_x_rep in x_rep:\n",
        "    plt.figure(figsize=(15, 15), frameon=False)\n",
        "    \n",
        "    for i in range (this_x_rep.shape[2]):\n",
        "        val = this_x_rep[:,:,i]\n",
        "        plt.subplot(6, 6, i + 1)\n",
        "        plt.axis(\"off\")\n",
        "        plt.imshow(val, cmap='gray',interpolation='nearest')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OhErkuAD1laZ"
      },
      "source": [
        "#STEP 13: 마지막 인공지능 모델 (VGG-like CNN)\n",
        "\n",
        "![대체 텍스트](https://neurohive.io/wp-content/uploads/2018/11/vgg16-1-e1542731207177.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zBIhP_xVFock"
      },
      "source": [
        "def cnn_model():\n",
        "    # create model\n",
        "    model = Sequential()\n",
        "    \n",
        "    model.add(Conv2D(32, (3,3), input_shape=(28, 28, 1)))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation(activation='relu'))\n",
        "    \n",
        "    model.add(Conv2D(32, (3,3)))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation(activation='relu'))\n",
        "    \n",
        "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "    \n",
        "    model.add(Conv2D(64, (3,3)))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation(activation='relu'))\n",
        "    \n",
        "    model.add(Conv2D(64, (3,3)))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation(activation='relu'))\n",
        "    \n",
        "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "    \n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(128, activation='relu'))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(num_classes, activation='softmax'))\n",
        "    \n",
        "    # Compile model\n",
        "    sgd = optimizers.SGD(learning_rate=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
        "    model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
        "    \n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3OB5CedfFocl"
      },
      "source": [
        "# build the model\n",
        "model = cnn_model()\n",
        "\n",
        "# fix random seed for reproductibility\n",
        "seed = 7\n",
        "np.random.seed(seed)\n",
        "\n",
        "# Fit the model\n",
        "hist = model.fit(trainX, trainY, validation_data=(valX, valY), epochs=20, batch_size=64, verbose=2)\n",
        "model.save('cnn_model.h5')\n",
        "\n",
        "# 5. 학습과정 살펴보기\n",
        "fig, loss_ax = plt.subplots()\n",
        "\n",
        "acc_ax = loss_ax.twinx()\n",
        "\n",
        "loss_ax.plot(hist.history['loss'], 'y', label='train loss')\n",
        "loss_ax.plot(hist.history['val_loss'], 'r', label='val loss')\n",
        "loss_ax.set_ylim([0.0, 1.5])\n",
        "\n",
        "acc_ax.plot(hist.history['accuracy'], 'b', label='train acc')\n",
        "acc_ax.plot(hist.history['val_accuracy'], 'g', label='val acc')\n",
        "acc_ax.set_ylim([0.5, 1.0])\n",
        "\n",
        "loss_ax.set_xlabel('epoch')\n",
        "loss_ax.set_ylabel('loss')\n",
        "acc_ax.set_ylabel('accuray')\n",
        "\n",
        "loss_ax.legend(loc='upper left')\n",
        "acc_ax.legend(loc='lower left')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FR3ZmGKkFocn"
      },
      "source": [
        "# Final evaluation of the model\n",
        "scores = model.evaluate(testX, testY, verbose=0)\n",
        "print(\"VGG-like CNN error: %.2f%%\" % (100-scores[1]*100))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BM17JvSG1_y_"
      },
      "source": [
        "#STEP 14: 결과 확인하기 (틀린 것 들만)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yMnR251GFocp"
      },
      "source": [
        "# 7. 모델 사용하기\n",
        "yhat_test = model.predict(testX, batch_size=32)\n",
        "\n",
        "plt_row = 5\n",
        "plt_col = 5\n",
        "\n",
        "plt.rcParams[\"figure.figsize\"] = (20,20)\n",
        "\n",
        "f, axarr = plt.subplots(plt_row, plt_col)\n",
        "\n",
        "cnt = 0\n",
        "i = 0\n",
        "\n",
        "while cnt < (plt_row*plt_col):\n",
        "    \n",
        "    if np.argmax(testY[i]) == np.argmax(yhat_test[i]):\n",
        "        i += 1\n",
        "        continue\n",
        "    \n",
        "    sub_plt = axarr[(int)(cnt/plt_row), cnt%plt_col]\n",
        "    sub_plt.axis('off')\n",
        "    sub_plt.imshow(testX[i].reshape(width, height), cmap='gray')\n",
        "    sub_plt_title = 'R: ' + labelNames[np.argmax(testY[i])] + '(%.2f)'% (yhat_test[i][np.argmax(testY[i])]) + ' P: ' + labelNames[np.argmax(yhat_test[i])] + '(%.2f)'% (  yhat_test[i][np.argmax(yhat_test[i])])\n",
        "    sub_plt.set_title(sub_plt_title)\n",
        "\n",
        "    i += 1    \n",
        "    cnt += 1\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}